[project]
name = "introspective-awareness"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    # Core PyTorch
    "torch==2.7.0",
    "torchaudio==2.7.0",
    "torchvision==0.22.0",
    # CUDA / cuDNN / related NVIDIA libs
    "nvidia-cublas-cu12==12.6.4.1",
    "nvidia-cuda-cupti-cu12==12.6.80",
    "nvidia-cuda-nvrtc-cu12==12.6.77",
    "nvidia-cuda-runtime-cu12==12.6.77",
    "nvidia-cudnn-cu12==9.5.1.17",
    "nvidia-cufft-cu12==11.3.0.4",
    "nvidia-cufile-cu12==1.11.1.6",
    "nvidia-curand-cu12==10.3.7.77",
    "nvidia-cusolver-cu12==11.7.1.2",
    "nvidia-cusparse-cu12==12.5.4.2",
    "nvidia-cusparselt-cu12==0.6.3",
    "nvidia-nccl-cu12==2.26.2",
    "nvidia-nvjitlink-cu12==12.6.85",
    "nvidia-nvtx-cu12==12.6.77",
    # Optional CUDA utility + kernel compilation tools
    "triton==3.3.0",
    "cupy-cuda12x==13.5.1",
    "fastrlock==0.8.3",
    # Flash Attention (custom wheel below)
    "flash-attn",
    "ipykernel>=7.1.0",
    "python-dotenv>=1.2.1",
    "tqdm>=4.67.1",
    "pandas>=2.3.3",
    "datasets>=4.4.1",
    "scipy>=1.16.3",
    "scikit-learn>=1.7.2",
    "seaborn>=0.13.2",
    "openai>=2.7.1",
    "rollouts>=0.1.11",
    "anthropic>=0.72.0",
    "matplotlib>=3.10.7",
    "accelerate>=1.11.0",
    "packaging>=25.0",
    "ninja>=1.13.0",
    "cpufeature>=0.2.1",
]

[tool.uv.sources]
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0+cu124torch2.7-cp311-cp311-linux_x86_64.whl" }

